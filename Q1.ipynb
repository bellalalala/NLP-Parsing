{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file into dict{file:[list of sentence]} \n",
    "import glob, re\n",
    "read_files = glob.glob(\"corpus\\\\*.txt\")\n",
    "corpus = dict.fromkeys(read_files)\n",
    "for i in range(len(read_files)):\n",
    "    with open(read_files[i], \"r\") as infile:\n",
    "        corpus[read_files[i]]  = infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install stanfordcorenlp first\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP(r'stanford-corenlp-full-2018-10-05', memory='8g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split sentences in each file\n",
    "props={'annotators': 'ssplit','pipelineLanguage':'en','outputFormat':'json'}\n",
    "import json\n",
    "corpus_s = dict.fromkeys(corpus)\n",
    "for key, value in corpus.items():\n",
    "    corpus_s[key]=[]\n",
    "    test = json.loads(nlp.annotate(value, properties=props))\n",
    "    for i in range(len(test['sentences'])):\n",
    "        word_l = []\n",
    "        for j in range(len(test['sentences'][i]['tokens'])):\n",
    "            word_l.append(test['sentences'][i]['tokens'][j]['word'])\n",
    "        corpus_s[key].append(' '.join(word_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations, multiple spaces\n",
    "import string\n",
    "re_p = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "corpus_cl = dict.fromkeys(corpus_s)\n",
    "for key, value in corpus_s.items():\n",
    "    corpus_cl[key] = []\n",
    "    for i in range(len(value)):\n",
    "        corpus_cl[key].append(re.sub(r' {2,}',' ', re_p.sub(' ', value[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count words in each sentence\n",
    "word_count = dict.fromkeys(corpus)\n",
    "for key, value in corpus_cl.items():\n",
    "    word_count[key] = []\n",
    "    for i in range(len(value)):\n",
    "        word_count[key].append(len(list(filter(None, value[i].split(' ')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the sentence with <=50 words\n",
    "corpus_rdtu = dict.fromkeys(corpus_cl)\n",
    "for key, value in corpus_s.items():\n",
    "    corpus_rdtu[key] = []\n",
    "    for i in range(len(value)):\n",
    "        if word_count[key][i] <=50:\n",
    "            corpus_rdtu[key].append(value[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag = dict.fromkeys(corpus_rdtu)\n",
    "for key, value in corpus_rdtu.items():\n",
    "    pos_tag[key]=[]\n",
    "    for i in range(len(value)):\n",
    "        pos_tag[key].append(nlp.pos_tag(value[i]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_parsing = dict.fromkeys(corpus_rdtu)\n",
    "for key, value in corpus_rdtu.items():\n",
    "    c_parsing[key]=[]\n",
    "    for i in range(len(value)):\n",
    "        c_parsing[key].append(nlp.parse(value[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_parsing = dict.fromkeys(corpus_rdtu)\n",
    "for key, value in corpus_rdtu.items():\n",
    "    d_parsing[key]=[]\n",
    "    for i in range(len(value)):\n",
    "        d_parsing[key].append(nlp.dependency_parse(value[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token{file1:[[sentence1_token_1,sentence1_token_2,...],[sentence2],...], file2:[[s1][s2]...],...}\n",
    "tkn = dict.fromkeys(corpus_rdtu)\n",
    "for key, value in corpus_rdtu.items():\n",
    "    tkn[key]=[]\n",
    "    for i in range(len(value)):\n",
    "        tkn[key].append(nlp.word_tokenize(value[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the intermediate files\n",
    "import pickle\n",
    "with open('pos_tag.pkl', 'wb+') as f:\n",
    "    pickle.dump(pos_tag, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('c_parsing.pkl', 'wb+') as f:\n",
    "    pickle.dump(c_parsing, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('d_parsing.pkl', 'wb+') as f:\n",
    "    pickle.dump(d_parsing, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('tkn.pkl', 'wb+') as f:\n",
    "    pickle.dump(tkn, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to output format postag\n",
    "#postag{file1:[[sentence1],[sentence2],...], file2:[[s1][s2]...],...}\n",
    "pos_tag_text = dict.fromkeys(pos_tag)\n",
    "for key, value in pos_tag.items():\n",
    "    pos_tag_text[key] = []\n",
    "    for i in range(len(value)):\n",
    "        pos_list_raw = []\n",
    "        for j in range(len(value[i])):\n",
    "            pos_list_raw.append('/'.join(value[i][j]))\n",
    "        pos_tag_text[key].append(' '.join(pos_list_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to output format d_parser\n",
    "#d_parser_op{file1:[[sentence1_token_1,sentence1_token_2,...],[sentence2],...], file2:[[s1][s2]...],...}\n",
    "d_parser_op = dict.fromkeys(d_parsing)\n",
    "for key, value in d_parsing.items():\n",
    "    d_parser_op[key] = []\n",
    "    for i in range(len(value)):\n",
    "        d_list_raw = []\n",
    "        for j in range(len(value[i])):\n",
    "            if value[i][j][1] >0: \n",
    "                d_list_raw.append('(%s, %s-%s, %s-%s)' % (value[i][j][0],tkn[key][i][value[i][j][1]-1], value[i][j][1], tkn[key][i][value[i][j][2]-1], value[i][j][2]))\n",
    "            else:\n",
    "                d_list_raw.append('(%s, ROOT-%s, %s-%s)' % (value[i][j][0], value[i][j][1], tkn[key][i][value[i][j][2]-1], value[i][j][2]))\n",
    "        d_parser_op[key].append(d_list_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "for key, value in pos_tag_text.items():\n",
    "    with open('output\\\\%s' % key[-8:], 'w+') as f:\n",
    "        for i in range(len(value)):\n",
    "            f.write('%s \\n %s \\n %s\\n\\n' % (value[i], c_parsing[key][i], d_parser_op[key][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1 Average Verb Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the intermediate files\n",
    "import pickle\n",
    "with open('pos_tag.pkl', 'rb+') as f:\n",
    "    pos_tag = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of verbs: 3.6505163928744713\n"
     ]
    }
   ],
   "source": [
    "c_v = 0\n",
    "c_s = 0\n",
    "verb_tags = []\n",
    "for key, value in pos_tag.items():\n",
    "    for i in range(len(value)):\n",
    "        for j in range(len(value[i])):\n",
    "            if value[i][j][1][0] == 'V':\n",
    "                verb_tags.append(value[i][j][1])\n",
    "                c_v += 1\n",
    "    c_s += len(value)\n",
    "v_avg_count = c_v/c_s\n",
    "print('average number of verbs: %s' % v_avg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb_tags: {'VBD', 'VB', 'VBN', 'VBG', 'VBZ', 'VBP'}\n"
     ]
    }
   ],
   "source": [
    "### After looking at the penn treebank, all the tags start with 'V' are verbs:\n",
    "print('verb_tags: %s' % set(verb_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2 Number of Sentences Parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the intermediate files\n",
    "import pickle\n",
    "with open('d_parsing.pkl', 'rb+') as f:\n",
    "    d_parsing = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=0\n",
    "for key, value in d_parsing.items():\n",
    "    for i in range(len(value)):\n",
    "        for j in range(len(value[i])):\n",
    "            if value[i][j][0] == 'ROOT':\n",
    "                s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of sentences parsed: 14427\n"
     ]
    }
   ],
   "source": [
    "print(\"The total number of sentences parsed: %s\" % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3 Total Number of Prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the intermediate files\n",
    "import pickle\n",
    "with open('d_parsing.pkl', 'rb+') as f:\n",
    "    d_parsing = pickle.load(f)\n",
    "with open('tkn.pkl', 'rb+') as f:\n",
    "    tkn = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_count = dict.fromkeys(d_parsing)\n",
    "for key, value in d_parsing.items():\n",
    "    prep_count[key] = 0\n",
    "    for i in range(len(value)):\n",
    "        for j in range(len(value[i])):\n",
    "            if value[i][j][0] == 'case':\n",
    "                prep_count[key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "with open('prep_count.txt', 'w+') as f:\n",
    "    for key, value in prep_count.items():\n",
    "        f.write('%s: %s \\n' % (key[-8: ],value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep= []\n",
    "for key, value in d_parsing.items():\n",
    "    for i in range(len(value)):\n",
    "        for j in range(len(value[i])):\n",
    "            if value[i][j][0] == 'case':\n",
    "                prep.append(tkn[key][i][value[i][j][2]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_count = dict.fromkeys(set(prep))\n",
    "for key in prep_count:\n",
    "    prep_count[key] = 0\n",
    "for key, value in d_parsing.items():\n",
    "    for i in range(len(value)):\n",
    "        for j in range(len(value[i])):\n",
    "            if value[i][j][0] == 'case':\n",
    "                prep_count[tkn[key][i][value[i][j][2]-1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common 3 prepositions are 'of', 'in', 'to'.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "k = Counter(prep_count) \n",
    "# Finding 3 highest values \n",
    "high = k.most_common(3) \n",
    "print(\"The most common 3 prepositions are '%s', '%s', '%s'.\" % (high[0][0],high[1][0],high[2][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.4 Errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constituent Parser:\n",
    "Ambiguity: Two common kinds of ambiguity are attachment ambiguity and coordination ambiguity. A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place. In coordination ambiguity different sets of phrases can be conjoined by a conjunction like \"and\". The problem can be solved by a probabilistic parser. Probabilistic parsers compute the probability of each interpretation and choose the most probable interpretation.\n",
    "\n",
    "Lack of lexical preferences: The problem can be solved by modifying the probabilistic model of the parser to\n",
    "allow for lexicalized rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dpendency Parser:\n",
    "computational limitations: The transition-based approaches can only produce projective trees, hence any sentences with non-projective structures will necessarily contain some errors. The problem can be solved by more flexible graph-based parsing approaches.\n",
    "Rely too much on verbs: Every node needs a head and the whole parser rely on verbs. Dependency grammar approach abstracts away from word-order information, representing only the information that is necessary for the parse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
